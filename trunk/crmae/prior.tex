%[ADWAIT, MORE STT-RAM RELATED WORK other than HPCA 2011 paper ESPECIALLY FOR STT-RAM CACHE DESIGN should be put here]

%Liang et al.~\cite{3t1d-cache} studied the impact of process variation on a 3T1D
%L1 data cache and modeled the process variations as variations in data retention time.

%Below, we discuss the prior works most relevant to our work.

%This section summarizes the prior work closely related to ours.
This section summarizes the circuit and architectural techniques proposed for enhancing the STT-RAM
write performance.

The work that is most closely related to ours is~\cite{STTRAM:HPCA11}.
Here, the authors relax the retention time of STT-RAM from $10 years$ to $56 \mu s$ by
reducing the planar area of MTJ from $32F^2$ to $10F^2$. However this methodology is limited by 
technology challenges in newer scaled MTJs. Recently published work related to
STT-RAM~\cite{PMTJ:Toshiba08,STTRAM:EDL11,STTRAM:Qualcomm09,STTRAM:Grandis11}
uses state-of-the-art MTJ designs in the range of 2-3$F^2$. Since, these designs don't give the
leeway of reducing the retention time by aggressively reducing the MTJ planar area as in~\cite{STTRAM:HPCA11} , we reduce the retention time by lowering the saturation magnetization and the thickness of the free layer. Analysis of retention times of LLC blocks with 25 multi-threaded and multi-programmed applications, shows that
the ideal retention time of LLC should be in the order of $ms$ and not in {\it $\mu$s} as proposed
in~\cite{STTRAM:HPCA11}. Moreover, the thickness of the free layer for the MTJs in our work is 
around $2nm$, and reducing the thickness further
to get {\it $\mu$s} retention time, will lead into MTJ reliability issues
and make it more vulnerable to process variations. Also, our proposed refresh scheme is simple, yet very performance and energy efficient compared to the DRAM-style refreshing proposed in~\cite{STTRAM:HPCA11}.

The 3T1D designs proposed in ~\cite{3T-brooks} has typical worst-case retention time in {\it $\mu$s} region, which makes it incompatible for our LLC design that needs $ms$ retention time. Increasing the retention time of 3T1D cell to $ms$ region will enlarge the size of the gated-diode or will increase the threshold voltage of the access transistor. These choices will incur significant area overhead or performance degradation for our cache design, respectively. 


A very recent effort~\cite{multi-level-retention}explores the possibility of designing LLC 
with STT-RAM banks of varying retention times and moving dying blocks 
from lowest retention time ({\it $\mu$s}) bank to the higher 
ones~\cite{multi-level-retention}. In contrast to this work that 
populates STT-RAM cells of different retention times, we tune our design 
for single retention times across the memory hierarchy. Our approach 
eases the challenge of higher die costs and yield issues associated with 
heterogeneous retention cells and irregular structures. Further, the {\it $\mu$s}
retention times used in the prior effort are challenging to achieve in 
newer, scaled MTJ dimensions as indicated earlier. Unlike recent STT-RAM papers 
that assume that the non-active STT-RAM banks are power gated, we conservatively assume that
these banks leak. Consequently, our absolute leakage numbers are larger.

A few other prior works have also proposed architectural and circuit level
solutions to handle this long write latency problem in STT-RAMs. Architectural techniques such as
early write termination~\cite{mram-energy-reduction}, hybrid SRAM/STT-RAM
architecture~\cite{gsun-hpca, Qureshi:2009:SHPMM} and read-preemptive write-buffer designs have been
shown to mitigate write latency/energy. The circuit level techniques such as eliminating redundant
bit-writes~\cite{mram-energy-reduction} and data inverting technique~\cite{gsun-hpca} have also been
shown to be effective in hiding the long write latency. In contrast to all these prior works that
attempt to {\it hide} the write latency, our scheme investigates techniques to {\it actually} reduce
the write latency of STT-RAM banks and make their write latency comparable to SRAM banks. When
compared to Zhou et al.'s work~\cite{mram-energy-reduction} that require additional gates for
detection and termination of writes inside {\it each STT-RAM sub-bank}, our techniques are simpler to
implement since our proposal works at a much coarser granularity.

Sun et al.~\cite{gsun-hpca} showed that write buffers can be helpful in hiding the long write
latencies of STT-RAM banks. Our analysis shows that, if an application is bursty, write-buffers fail
to hide this latency and are rendered in-effective. Out of 25 applications, we found 12 applications
to be write intensive and bursty and hence, write-buffering is ineffective for these applications.
Moreover, all our results are conservative since we have already assumed a 10-entry (as used
in~\cite{gsun-hpca}) write-buffer at every STT-RAM bank and our results would be significantly better
without the presence of write-buffers.

In a recent work~\cite{mram-noc}, the authors have proposed a network level solution to hide the
write latency of STT-RAM banks. This solution requires complex busy/idle bank detection followed by
prioritization mechanisms in the network. On a qualitative basis, the network level solution to hide
write latency in \cite{mram-noc} was shown as the most promising technique compared to any other
techniques. The application level performance improvement with this scheme was about 2-4\% higher
compared to the write buffering technique of Sun et al.~\cite{gsun-hpca}. Contrasting this to our
work here, our scheme provides about 15\%/4\%(PARSEC IPC/SPEC weighted-speedup) improvement over $10years$
traditional STT-RAM, on top of the write buffering scheme, thereby making it more attractive compared
to \cite{mram-noc}. Overall, we believe that no prior work makes a case for tuning the retention time
of STT-RAM banks that is based on profiling retention duration of last-level cache blocks of
applications, which our proposal does.
